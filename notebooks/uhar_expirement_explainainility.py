# -*- coding: utf-8 -*-
"""UHAR_Expirement_Explainainility.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j92aMpDBaLx8WGEWqjQEai_OSsKgwkUF
"""

import os
import json
import logging
import random
import torch
import timm
import cv2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
from PIL import Image
from torchvision import datasets, transforms
from sklearn.metrics import auc
from tqdm.auto import tqdm

from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image

# Suppress warnings from the visualization library
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

CONFIG = {
    "data_path": "/content/uhar/data/data",
    "checkpoints_dir": "/content/drive/MyDrive/uhar/checkpoints/",
    "xai_results_dir": "/content/drive/MyDrive/uhar/xai_results/",
    "num_classes": 40,
    "image_size": 224,
    "models_to_explain": [
        "resnet18",
        "efficientnet_b0",
        "deit_tiny_distilled_patch16_224",
        "swin_tiny_patch4_window7_224"
    ]
}

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
os.makedirs(CONFIG["xai_results_dir"], exist_ok=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler()])
logging.info(f"Using device: {DEVICE}")

# Unzip data if not already present
if not os.path.exists(CONFIG["data_path"]):
    !unzip -q "/content/archive (4).zip" -d "/content/uhar"

def create_test_dataset_pil(config):
    """Creates a test dataset that returns PIL images and handles class name inconsistencies."""
    data_transform = transforms.Compose([
        transforms.Resize((config["image_size"], config["image_size"])),
        transforms.Grayscale(num_output_channels=3),
    ])
    train_dir = os.path.join(config["data_path"], "characters_train_set")
    test_dir = os.path.join(config["data_path"], "characters_test_set")

    train_dataset_for_mapping = datasets.ImageFolder(train_dir)
    class_to_idx = train_dataset_for_mapping.class_to_idx
    class_names = train_dataset_for_mapping.classes

    # Load test set with a loader that returns PIL images
    test_dataset = datasets.ImageFolder(test_dir, loader=lambda x: Image.open(x).convert("RGB"))

    # Remap test set labels to match training set, handling case differences
    class_to_idx_lower = {k.lower(): v for k, v in class_to_idx.items()}
    valid_imgs = []
    for path, _ in test_dataset.imgs:
        class_name_from_folder = os.path.basename(os.path.dirname(path)).lower()
        correct_idx = class_to_idx_lower.get(class_name_from_folder)
        if correct_idx is not None:
            valid_imgs.append((path, correct_idx))

    test_dataset.imgs = valid_imgs
    test_dataset.samples = valid_imgs
    test_dataset.targets = [s[1] for s in valid_imgs]

    logging.info(f"Test dataset created with {len(test_dataset)} images across {len(class_names)} classes.")
    return test_dataset, class_names

test_dataset, class_names = create_test_dataset_pil(CONFIG)

@torch.no_grad()
def get_model(model_name, config):
    """loads a trained model from a checkpoint and sets it to evaluation mode."""
    checkpoint_path =  f"{model_name}_best_model.pth"
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found for {model_name} at {checkpoint_path}")

    model = timm.create_model(model_name, pretrained=False, num_classes=config["num_classes"])
    model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))
    model.eval()
    return model.to(DEVICE)

CNN_TARGET_LAYERS = {
    "resnet18": lambda m: m.layer4[-1],
    "resnet50": lambda m: m.layer4[-1],
    "mobilenetv2_100": lambda m: m.conv_head,
    "efficientnet_b0": lambda m: m.conv_head,
    "swin_tiny_patch4_window7_224": lambda m: m.layers[-1].blocks[-1],
    # ADD THIS LINE for the DeiT model
    "deit_tiny_distilled_patch16_224": lambda m: m.blocks[-1].norm1
}

# Define the preprocessing transform for model input
preprocess_transform = transforms.Compose([
    transforms.Resize((CONFIG["image_size"], CONFIG["image_size"])),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def generate_grad_cam(model, image_tensor, target_layer_fn):
    """Generates a Grad-CAM heatmap and predicted class index for a CNN model."""
    target_layer = target_layer_fn(model)
    cam = GradCAM(model=model, target_layers=[target_layer])

    with torch.no_grad():
        target_category = model(image_tensor).argmax(dim=1).item()

    targets = [ClassifierOutputTarget(target_category)]
    grayscale_cam = cam(input_tensor=image_tensor, targets=targets)[0, :]
    return grayscale_cam, target_category

'''def generate_attention_rollout(model, image_tensor):
    """Generates an attention rollout map and predicted class index for a ViT model."""
    with torch.no_grad():
        target_category = model(image_tensor).argmax(dim=1).item()

    attentions = []
    hooks = [
        # This hook path is correct for DeiT models from timm
        block.attn.attn_drop.register_forward_hook(lambda module, input, output: attentions.append(output))
        for block in model.blocks
    ]

    # --- FIX ---
    # Temporarily switch to train() mode to ensure the dropout hooks fire.
    # No gradients are computed, so this does not affect weights.
    model.train()
    with torch.no_grad():
        model(image_tensor)
    # Immediately return to evaluation mode
    model.eval()
    # --- END FIX ---

    for hook in hooks:
        hook.remove()

    # Add a check to prevent crashing if no attention maps were captured
    if not attentions:
        raise RuntimeError("ERROR: Could not capture any attention maps from the model. "
                           "The hook on 'block.attn.attn_drop' is likely incorrect for this model's architecture.")

    result = torch.eye(attentions[0].shape[-1], device=DEVICE)
    for attention in attentions:
        attention_heads_fused = attention.mean(axis=1)
        I = torch.eye(attention_heads_fused.shape[-1], device=DEVICE)
        a = (attention_heads_fused + I) / 2
        a = a / a.sum(dim=-1, keepdim=True)
        result = torch.matmul(a, result)

    mask = result[0, 0, 1:]
    grid_size = int(np.sqrt(mask.shape[-1]))
    mask = mask.reshape(grid_size, grid_size).cpu().numpy()
    return mask / mask.max(), target_category'''

def calculate_deletion_auc(model, image_tensor, explanation_map, true_class_idx):
    """Calculates the Deletion Area Under the Curve (AUC). Lower is better."""
    num_pixels = image_tensor.shape[-2] * image_tensor.shape[-3]
    flat_map = explanation_map.flatten()
    sorted_indices = np.argsort(flat_map)[::-1]

    image_flat = image_tensor.clone().cpu().flatten().numpy() # Convert to numpy array
    confidences = []

    with torch.no_grad():
        initial_prob = F.softmax(model(image_tensor), dim=1)[0, true_class_idx].item()
    confidences.append(initial_prob)

    steps = np.linspace(0, num_pixels - 1, num=21, dtype=int)
    for num_to_delete in steps[1:]:
        img_copy_flat = image_flat.copy() # Use numpy copy
        img_copy_flat[sorted_indices[:num_to_delete]] = 0
        # Convert back to tensor, reshape, and move to device
        img_tensor = torch.from_numpy(img_copy_flat).reshape_as(image_tensor.cpu()).to(DEVICE)

        with torch.no_grad():
            prob = F.softmax(model(img_tensor), dim=1)[0, true_class_idx].item()
        confidences.append(prob)

    x_axis = np.linspace(0, 1, len(confidences))
    return auc(x_axis, confidences)

from io import BytesIO; from PIL import ImageFilter
def add_gaussian_noise(img, sigma):
  arr = np.array(img).astype(np.float32)
  noise = np.random.normal(0, sigma, arr.shape)
  return Image.fromarray(np.clip(arr + noise, 0, 255).astype(np.uint8))
def add_salt_and_pepper(img, prob): arr = np.array(img).copy(); h, w = arr.shape[:2]; num = int(h*w*prob); [np.put(arr, [np.random.randint(0,h), np.random.randint(0,w), i], 0 if random.random()<0.5 else 255) for i in range(arr.shape[2])] if arr.ndim == 3 else [np.put(arr, [np.random.randint(0,h), np.random.randint(0,w)], 0 if random.random()<0.5 else 255) for _ in range(num)]; return Image.fromarray(arr)
def motion_blur(img, radius):
  """
  """
  # Convert PIL Image to NumPy array
  arr = np.array(img)

  # CV2's GaussianBlur needs an odd kernel size. We'll derive it from the radius.
  # A larger radius means more blur.
  kernel_size = int(radius)
  if kernel_size % 2 == 0:
      kernel_size += 1

  # Apply the blur
  blurred_arr = cv2.GaussianBlur(arr, (kernel_size, kernel_size), 0)

  # Convert back to PIL Image
  return Image.fromarray(blurred_arr)


def find_case_study_image(robust_model, fragile_model, corruption_fn, severity, test_dataset, max_tries=500):
    """Finds an image that the robust model gets right and the fragile model gets wrong."""
    logging.info(f"Searching for case study: Robust={robust_model.name}, Fragile={fragile_model.name}...")

    for i in range(max_tries):
        pil_img, true_idx = random.choice(test_dataset.samples)

        corrupted_pil = corruption_fn(pil_img, severity)
        corrupted_tensor = preprocess_transform(corrupted_pil).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            robust_pred = robust_model(corrupted_tensor).argmax().item()
            fragile_pred = fragile_model(corrupted_tensor).argmax().item()

        if robust_pred == true_idx and fragile_pred != true_idx:
            logging.info(f"Found suitable image at index {i} for class '{class_names[true_idx]}'.")
            return pil_img, corrupted_pil, true_idx

    logging.warning("Could not find a suitable case study image after many tries.")
    return None, None, None

all_results = []
models_loaded = {name: get_model(name, CONFIG) for name in CONFIG["models_to_explain"]}

case_studies = {
    "RobustCNN_vs_FragileCNN_Gaussian": {
        "robust": "resnet18", "fragile": "efficientnet_b0", "models_to_show": ["resnet18", "efficientnet_b0"],
        "corruption_fn": add_gaussian_noise, "severity": 20, "corruption_info": "Gaussian Noise (Sev 20)"
    },
    "RobustViT_vs_FragileCNN_SaltPepper": {
        "robust": "deit_tiny_distilled_patch16_224", "fragile": "efficientnet_b0", "models_to_show": ["deit_tiny_distilled_patch16_224", "efficientnet_b0"],
        "corruption_fn": add_salt_and_pepper, "severity": 0.06, "corruption_info": "Salt & Pepper (Sev 0.06)"
    },
    "MotionBlur_Specialist_vs_Generalist": {
        "robust": "swin_tiny_patch4_window7_224", "fragile": "resnet18", "models_to_show": ["swin_tiny_patch4_window7_224", "resnet18"],
        "corruption_fn": motion_blur, "severity": 15, "corruption_info": "Motion Blur (Sev 15)"
    }
}

def find_case_study_image(robust_model_name, fragile_model_name, robust_model, fragile_model, corruption_fn, severity, test_dataset, max_tries=500):
    logging.info(f"Searching for case study: Robust={robust_model_name}, Fragile={fragile_model_name}...")
    for i in range(max_tries):
        image_path, true_idx = random.choice(test_dataset.samples)
        pil_img = Image.open(image_path).convert("RGB")
        corrupted_pil = corruption_fn(pil_img, severity)
        corrupted_tensor = preprocess_transform(corrupted_pil).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            robust_pred = robust_model(corrupted_tensor).argmax().item()
            fragile_pred = fragile_model(corrupted_tensor).argmax().item()
        if robust_pred == true_idx and fragile_pred != true_idx:
            logging.info(f"Found suitable image for class '{class_names[true_idx]}'.")
            return pil_img, corrupted_pil, true_idx
    logging.warning("Could not find a suitable case study image after many tries.")
    return None, None, None


# --- Main processing and visualization loop (FINAL VERSION) ---
for name, params in case_studies.items():
    robust_model = models_loaded[params["robust"]]
    fragile_model = models_loaded[params["fragile"]]
    clean_pil, corrupted_pil, true_idx = find_case_study_image(
        params["robust"], params["fragile"], robust_model, fragile_model,
        params["corruption_fn"], params["severity"], test_dataset
    )
    if not clean_pil: continue

    corrupted_tensor = preprocess_transform(corrupted_pil).unsqueeze(0).to(DEVICE)
    num_models_to_show = len(params["models_to_show"])
    fig, axes = plt.subplots(num_models_to_show, 3, figsize=(12, 4 * num_models_to_show))
    if num_models_to_show == 1: axes = np.array([axes])
    fig.suptitle(f"Case Study: {name}\nCorruption: {params['corruption_info']} | True Label: {class_names[true_idx]}", fontsize=16)

    for i, model_name in enumerate(params["models_to_show"]):
        model = models_loaded[model_name]

        with torch.no_grad():
            corrupted_pred_idx = model(corrupted_tensor).argmax(dim=1).item()
        targets = [ClassifierOutputTarget(corrupted_pred_idx)]

        target_layer = CNN_TARGET_LAYERS[model_name](model)
        reshape_transform = None

        if 'swin' in model_name:
            def reshape_transform(tensor):
                # The activation shape is (batch, 49, 768). Reshape to a 2D grid.
                result = tensor.reshape(tensor.size(0), 7, 7, 768) # HARDCODED FIX
                return result.permute(0, 3, 1, 2)
        elif 'deit' in model_name:
            def reshape_transform(tensor):
                # The activation shape is (batch, 198, 192). Remove class/dist tokens, then reshape.
                result = tensor[:, 2:, :]
                result = result.reshape(result.size(0), 14, 14, 192) # HARDCODED FIX
                return result.permute(0, 3, 1, 2)

        # NOTE: The use_cuda parameter might be deprecated. It's safe to remove if you get a warning.
        cam_algorithm = GradCAM(model=model, target_layers=[target_layer], reshape_transform=reshape_transform,)
        corrupted_map = cam_algorithm(input_tensor=corrupted_tensor, targets=targets)[0, :]

        # Visualization
        vis_map = cv2.resize(corrupted_map, (CONFIG["image_size"], CONFIG["image_size"]))
        del_auc = calculate_deletion_auc(model, corrupted_tensor, corrupted_map, true_idx)
        all_results.append({"Case": name, "Model": model_name, "Deletion AUC": del_auc, "Correct": class_names[corrupted_pred_idx] == class_names[true_idx]})

        img_for_overlay = np.array(corrupted_pil.resize((CONFIG["image_size"], CONFIG["image_size"]))) / 255.0
        overlay = show_cam_on_image(img_for_overlay, vis_map, use_rgb=True)
        axes[i, 0].imshow(corrupted_pil); axes[i, 0].set_title(f"Model: {model_name}\nCorrupted Input")
        axes[i, 1].imshow(vis_map, cmap='jet'); axes[i, 1].set_title("Saliency Map")
        axes[i, 2].imshow(overlay); axes[i, 2].set_title(f"Overlay\nPredicted: {class_names[corrupted_pred_idx]}")
        for ax in axes[i]: ax.axis('off')

    plt.tight_layout(rect=[0, 0, 1, 0.94])
    plt.savefig(os.path.join(CONFIG["xai_results_dir"], f"XAI_{name}.png"), dpi=300)
    plt.show()

!pip install opencv-python

results_df = pd.DataFrame(all_results)
pivot_df = results_df.pivot_table(index="Case", columns="Model", values=["Deletion AUC", "Correct"])

print("\n" + "="*80)
print("           Quantitative Explainability Summary (On Corrupted Images)")
print("="*80)
print("\n--- Explanation Fidelity (Deletion AUC - Lower is Better) ---")
print(pivot_df["Deletion AUC"].to_string(float_format="%.4f"))
print("\n--- Model Correctness on Selected Image ---")
print(pivot_df["Correct"])
print("="*80)

# Save the results to a CSV file
pivot_df.to_csv(os.path.join(CONFIG["xai_results_dir"], "quantitative_xai_summary.csv"))